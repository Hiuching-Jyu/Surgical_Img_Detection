from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
import os
from PIL import Image, ImageDraw, ImageFont
import torch
import json
torch.cuda.empty_cache()
import cv2

# <editor-fold desc="1. Recognition">
#os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # è®¾ç½®æ¨¡å‹åŠ è½½GPUï¼ˆ1å·GPUï¼‰
model_path = '/home/hiuching-g/PRHK/Qwen'  # ä¿®æ”¹ä¸ºæœ¬åœ°æ¨¡å‹ä¸‹è½½åœ°å€

# åŠ è½½æ¨¡å‹
# æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
img_path = "/home/hiuching-g/PRHK/test_images/surgery07_2615.png"
# ç›®æ ‡å®šä½æ¡†ä¿¡æ¯æé—®
question = """Identify and localise the following objects from the image:

(1) Surgical instruments like graspers, clip appliers, scissors, suction
(2) Relevant anatomic or pathologic targets like tumour mass, bleeding stump, necrotic tissue, mesenteric vessel

Return output in pure JSON array format (no markdown, no explanation, no prefix text). Each JSON object must contain these keys:

- "image_name": string
- "image_index": int
- "label": string
- "x1": int
- "y1": int
- "x2": int
- "y2": int

Coordinates are in absolute pixel values relative to the original image.

Output example:
[
  {"image_name": "[original name of the img].png", "image_index": 0, "label": "Grasper", "x1": 120, "y1": 90, "x2": 180, "y2": 160}
]
"""

# åŠ è½½æ¨¡å‹
#model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype="auto", device_map="auto")
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float16).to("cpu")

processor = AutoProcessor.from_pretrained(model_path, use_fast=True)
processor.save_pretrained(model_path)
# è¾“å…¥é…ç½®
image = Image.open(img_path)
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
            },
            {"type": "text", "text": question},
        ],
    }
]
text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors="pt")
# inputs = inputs.to('cuda')

# æ¨ç†
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
# </editor-fold>

# <editor-fold desc="2. Post-processing and saving results">
# output_text = ['[\n  {"image_name": "[original name of the img].png", "image_index": 0, "label": "Suction", "x1": 120, "y1": 90, "x2": 180, "y2": 160},\n  {"image_name": "[original name of the img].png", "image_index": 0, "label": "Scissors", "x1": 120, "y1": 90, "x2": 180, "y2": 160},\n ']
# Step 1: æå–å­—ç¬¦ä¸²
raw_json = output_text[0]

# Step 2: è¡¥ä¸Šç¼ºå¤±çš„ JSON ç»“å°¾ï¼Œå¹¶ç§»é™¤æœ«å°¾å¤šä½™é€—å·
raw_json = raw_json.strip()  # å»é™¤å‰åç©ºæ ¼å’Œæ¢è¡Œ
if raw_json.endswith(","):
    raw_json = raw_json[:-1]  # å»æ‰ç»“å°¾å¤šä½™é€—å·
if not raw_json.endswith("]"):
    raw_json += "]"  # ç¡®ä¿ç»“å°¾æœ‰ ]

# Step 3: å°†å…¶ä¸­çš„ [original name of the img] æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶å
img_filename = "surgery07_2615.png"  # æ ¹æ®ä½ çš„å›¾åƒè·¯å¾„å®é™…ä¿®æ”¹
raw_json = raw_json.replace("[original name of the img]", img_filename)

# # Step 4: å°è¯•è§£æ JSON
# try:
#     data = json.loads(raw_json)
#     print("âœ… JSON è§£ææˆåŠŸï¼")
# except json.JSONDecodeError as e:
#     print("âŒ JSON ä¸åˆæ³•:", e)

img_path = "/home/hiuching-g/PRHK/test_images/surgery07_2615.png"
json_path  = "/home/hiuching-g/PRHK/Output_Qwen/qwen.json"
out_dir    = "/home/hiuching-g/PRHK/Output_Qwen/"
os.makedirs(out_dir, exist_ok=True)

# Step 1: å°† output_text å†™å…¥ json æ–‡ä»¶
# clean_json_str = raw_json[0].replace("\n", "").replace("[original name of the img]", os.path.basename(img_path))
data = json.loads(raw_json)  # ç¡®ä¿å†…å®¹æ˜¯åˆæ³• JSON
image = Image.open(img_path)
print("ğŸ“ åŸå›¾å¤§å°:", image.size)
original_width, original_height = image.size  # PIL.Image åŸå›¾å¤§å°
model_input_size = processor.image_processor.size  # é€šå¸¸ä¸º dictï¼Œä¾‹å¦‚ {"height": 448, "width": 448}
print("ğŸ“ processor æ˜¯å¦æ”¹å˜å›¾åƒå°ºå¯¸ï¼Ÿ", model_input_size)

if "width" in model_input_size and "height" in model_input_size:
    model_width = model_input_size["width"]
    model_height = model_input_size["height"]
elif "shortest_edge" in model_input_size:
    model_width = model_height = model_input_size["shortest_edge"]
else:
    raise ValueError(f"Unrecognized image size format: {model_input_size}")

# ä¿®æ­£æ¯ä¸ª box çš„ä½ç½®
for obj in data:
    obj["x1"] = int(obj["x1"] / original_width * model_width)
    obj["y1"] = int(obj["y1"] / original_height * model_height)
    obj["x2"] = int(obj["x2"] / original_width * model_width)
    obj["y2"] = int(obj["y2"] / original_height * model_height)
with open(json_path, "w") as f:
    json.dump(data, f, indent=2)

# Step 2: åŠ è½½å›¾åƒå¹¶ç»˜åˆ¶æ ‡æ³¨
img = cv2.imread(img_path)
for obj in data:
    label = obj["label"]
    x1, y1, x2, y2 = obj["x1"], obj["y1"], obj["x2"], obj["y2"]

    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)
    cv2.putText(img, label, (x1, max(0, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)

# Step 3: ä¿å­˜è¾“å‡ºå›¾åƒ
out_path = os.path.join(out_dir, f"Qwen_annotated_{os.path.basename(img_path)}")
cv2.imwrite(out_path, img)
print(f"âœ… Saved annotated image to: {out_path}")

# </editor-fold>